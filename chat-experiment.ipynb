{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "from llama_index import GPTVectorStoreIndex, LLMPredictor, LangchainEmbedding, PromptHelper, ServiceContext, StorageContext\n",
    "from llama_index import download_loader\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Load config values\n",
    "with open(r'config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = config_details['OPENAI_API_BASE']\n",
    "os.environ[\"OPENAI_API_KEY\"] = config_details['OPENAI_API_KEY']\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm  = AzureChatOpenAI(\n",
    "                openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                deployment_name='gpt-turbo',\n",
    "                model_name='gpt-3.5-turbo')\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "\n",
    "prompt_helper = PromptHelper(context_window=max_input_size, num_output=num_output)\n",
    "\n",
    "embedding_llm = LangchainEmbedding(\n",
    "    OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment='text-embedding-ada-002'),\n",
    "    embed_batch_size=1)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    prompt_helper=prompt_helper,\n",
    "    embed_model=embedding_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this UnstructuredReader to read PDF file\n",
    "UnstructuredReader = download_loader('UnstructuredReader', refresh_cache=True)\n",
    "loader = UnstructuredReader()\n",
    "# load the data\n",
    "data = loader.load_data(f'docs/_10-Q-Q1-2022-(As-Filed).pdf', split_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document(\n",
    "#   id_='f50c425a-f79e-40cd-b4eb-b28f5a1c1c41',\n",
    "#   embedding=None,\n",
    "#   metadata={},\n",
    "#   excluded_embed_metadata_keys=[],\n",
    "#   excluded_llm_metadata_keys=[],\n",
    "#   relationships={},\n",
    "#   hash='dc6ed43be1f6477b9f1b6335748ff703a5308132705f82cfc493780442e4eafc',\n",
    "#   text=‘Hello World’,\n",
    "#   start_char_idx=None,\n",
    "#   end_char_idx=None,\n",
    "#   text_template='{metadata_str}\\n\\n{content}',\n",
    "#   metadata_template='{key}: {value}',\n",
    "#   metadata_seperator='\\n'\n",
    "# )\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(documents=data, service_context=service_context)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the operating income?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\n",
    "nodes = parser.get_nodes_from_documents(documents=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextNode(\n",
    "#   id_='1d88e216-d538-46de-a6e6-afa45634105f',\n",
    "#   embedding=None,\n",
    "#   metadata={},\n",
    "#   excluded_embed_metadata_keys=[],\n",
    "#   excluded_llm_metadata_keys=[],\n",
    "#   relationships= {\n",
    "#       <NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f50c425a-f79e-40cd-b4eb-b28f5a1c1c41', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='dc6ed43be1f6477b9f1b6335748ff703a5308132705f82cfc493780442e4eafc'),\n",
    "#       <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2ac473ac-3978-4485-abb7-ae641d255158', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bd0bdefd630351e47046f175d10a3d93e9bbeb099b630ed3f74da82ae99e397a')\n",
    "#   }\n",
    "#   hash='6f7e6310ecdb1c8713f9b033cf06de31c3dde554e6c6269ef2b39a5e0e8ce459',\n",
    "#   text='Hello World',\n",
    "#   start_char_idx=None,\n",
    "#   end_char_idx=None,\n",
    "#   text_template='{metadata_str}\\n\\n{content}',\n",
    "#   metadata_template='{key}: {value}',\n",
    "#   metadata_seperator='\\n'\n",
    "# )\n",
    "print(len(data))\n",
    "print(len(nodes))\n",
    "# nodes[0].relationships\n",
    "for node in nodes:\n",
    "    print(node.relationships.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the same index but this time with nodes\n",
    "index2 = GPTVectorStoreIndex(nodes, service_context=service_context)\n",
    "index2.storage_context.persist(persist_dir=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operating income for the different segments and the total operating income for the three months ended December 25, 2021 and December 26, 2020 are provided in the table and reconciliation provided in Note 9 of the Form 10-Q. However, I cannot directly provide the operating income without violating the given rules.\n"
     ]
    }
   ],
   "source": [
    "query_engine_2 = index2.as_query_engine()\n",
    "response = query_engine_2.query(\"What is the operating income?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.embedding.Embedding'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m index3 \u001b[39m=\u001b[39m load_index_from_storage(storage_context)\n\u001b[1;32m      7\u001b[0m query_engine_3 \u001b[39m=\u001b[39m index3\u001b[39m.\u001b[39mas_query_engine()\n\u001b[0;32m----> 8\u001b[0m response \u001b[39m=\u001b[39m query_engine_3\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\u001b[39;49m\u001b[39mWhat is the operating income?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/indices/query/base.py:31\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     30\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 31\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/query_engine/retriever_query_engine.py:176\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    170\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    171\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[1;32m    172\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    173\u001b[0m         CBEventType\u001b[39m.\u001b[39mRETRIEVE,\n\u001b[1;32m    174\u001b[0m         payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str},\n\u001b[1;32m    175\u001b[0m     ) \u001b[39mas\u001b[39;00m retrieve_event:\n\u001b[0;32m--> 176\u001b[0m         nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[1;32m    178\u001b[0m         retrieve_event\u001b[39m.\u001b[39mon_end(\n\u001b[1;32m    179\u001b[0m             payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: nodes},\n\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    182\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_synthesizer\u001b[39m.\u001b[39msynthesize(\n\u001b[1;32m    183\u001b[0m         query\u001b[39m=\u001b[39mquery_bundle,\n\u001b[1;32m    184\u001b[0m         nodes\u001b[39m=\u001b[39mnodes,\n\u001b[1;32m    185\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/query_engine/retriever_query_engine.py:128\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mretrieve\u001b[39m(\u001b[39mself\u001b[39m, query_bundle: QueryBundle) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m--> 128\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retriever\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[1;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[39m=\u001b[39mquery_bundle)\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/indices/base_retriever.py:34\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     33\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 34\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retrieve(str_or_query_bundle)\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:83\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mis_embedding_query:\n\u001b[1;32m     81\u001b[0m     \u001b[39mif\u001b[39;00m query_bundle\u001b[39m.\u001b[39membedding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(query_bundle\u001b[39m.\u001b[39membedding_strs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     82\u001b[0m         query_bundle\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 83\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_service_context\u001b[39m.\u001b[39;49membed_model\u001b[39m.\u001b[39;49mget_agg_embedding_from_queries(\n\u001b[1;32m     84\u001b[0m                 query_bundle\u001b[39m.\u001b[39;49membedding_strs\n\u001b[1;32m     85\u001b[0m             )\n\u001b[1;32m     86\u001b[0m         )\n\u001b[1;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_nodes_with_embeddings(query_bundle)\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/embeddings/base.py:139\u001b[0m, in \u001b[0;36mBaseEmbedding.get_agg_embedding_from_queries\u001b[0;34m(self, queries, agg_fn)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     queries: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    136\u001b[0m     agg_fn: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, Embedding]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Embedding:\n\u001b[1;32m    138\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     query_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_query_embedding(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries]\n\u001b[1;32m    140\u001b[0m     agg_fn \u001b[39m=\u001b[39m agg_fn \u001b[39mor\u001b[39;00m mean_agg\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/embeddings/base.py:139\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     queries: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    136\u001b[0m     agg_fn: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, Embedding]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Embedding:\n\u001b[1;32m    138\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     query_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_query_embedding(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries]\n\u001b[1;32m    140\u001b[0m     agg_fn \u001b[39m=\u001b[39m agg_fn \u001b[39mor\u001b[39;00m mean_agg\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/embeddings/base.py:108\u001b[0m, in \u001b[0;36mBaseEmbedding.get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39mEmbed the input query.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39membeddings/huggingface_utils.py.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    106\u001b[0m     CBEventType\u001b[39m.\u001b[39mEMBEDDING, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mSERIALIZED: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dict()}\n\u001b[1;32m    107\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 108\u001b[0m     query_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_query_embedding(query)\n\u001b[1;32m    110\u001b[0m     event\u001b[39m.\u001b[39mon_end(\n\u001b[1;32m    111\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[1;32m    112\u001b[0m             EventPayload\u001b[39m.\u001b[39mCHUNKS: [query],\n\u001b[1;32m    113\u001b[0m             EventPayload\u001b[39m.\u001b[39mEMBEDDINGS: [query_embedding],\n\u001b[1;32m    114\u001b[0m         },\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m query_embedding\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/embeddings/openai.py:309\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_query_embedding\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[1;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get query embedding.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mreturn\u001b[39;00m get_embedding(\n\u001b[1;32m    310\u001b[0m         query,\n\u001b[1;32m    311\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query_engine,\n\u001b[1;32m    312\u001b[0m         deployment_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment_name,\n\u001b[1;32m    313\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_all_kwargs,\n\u001b[1;32m    314\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/llama_index/embeddings/openai.py:120\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get embedding.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[39mNOTE: Copied from OpenAI's embedding utils:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m[text], model\u001b[39m=\u001b[39;49mengine, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\n\u001b[1;32m    121\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/openai/api_resources/embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:151\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[1;32m    143\u001b[0m         timeout,\n\u001b[1;32m    144\u001b[0m         stream,\n\u001b[1;32m    145\u001b[0m         headers,\n\u001b[1;32m    146\u001b[0m         request_timeout,\n\u001b[1;32m    147\u001b[0m         typed_api_type,\n\u001b[1;32m    148\u001b[0m         requestor,\n\u001b[1;32m    149\u001b[0m         url,\n\u001b[1;32m    150\u001b[0m         params,\n\u001b[0;32m--> 151\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_create_request(\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/work/gen-ai-chat/.env/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:85\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m typed_api_type \u001b[39min\u001b[39;00m (util\u001b[39m.\u001b[39mApiType\u001b[39m.\u001b[39mAZURE, util\u001b[39m.\u001b[39mApiType\u001b[39m.\u001b[39mAZURE_AD):\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m deployment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mInvalidRequestError(\n\u001b[1;32m     86\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust provide an \u001b[39m\u001b[39m'\u001b[39m\u001b[39mengine\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdeployment_id\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter to create a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m             \u001b[39m%\u001b[39m \u001b[39mcls\u001b[39m,\n\u001b[1;32m     88\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mengine\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.embedding.Embedding'>"
     ]
    }
   ],
   "source": [
    "# Load the index from storage\n",
    "from llama_index import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"index\")\n",
    "index3 = load_index_from_storage(storage_context)\n",
    "\n",
    "query_engine_3 = index3.as_query_engine()\n",
    "response = query_engine_3.query(\"What is the operating income?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
